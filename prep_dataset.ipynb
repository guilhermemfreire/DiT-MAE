{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Survey Items (Trend and New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_items = pd.read_excel('input_data/PISA2015_TechRep_Final-AnnexA.xlsx', sheet_name=None, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get item names from trend survey (3 sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNT', 'CNTSTUID', 'DS131Q02C', 'DS131Q04C', 'CS252Q01S', 'CS252Q02S', 'CS252Q03S', 'CS256Q01S', 'CS268Q01S', 'DS268Q02C', 'CS268Q06S', 'DS269Q01C', 'DS269Q03C', 'CS269Q04S', 'DS304Q01C', 'CS304Q02S', 'DS304Q03aC', 'DS304Q03bC', 'DS326Q01C', 'DS326Q02C', 'CS326Q03S', 'CS326Q04S', 'CS327Q01S', 'CS408Q01S', 'DS408Q03C', 'CS408Q04S', 'CS408Q05S', 'CS413Q04S', 'CS413Q05S', 'CS413Q06S', 'CS415Q02S', 'CS415Q07S', 'CS415Q08S', 'DS416Q01C', 'CS421Q01S', 'CS421Q02S', 'CS421Q03S', 'CS425Q02S', 'DS425Q03C', 'DS425Q04C', 'CS425Q05S', 'CS428Q01S', 'CS428Q03S', 'DS428Q05C', 'PS131Q02', 'PS131Q04', 'PS252Q01S', 'PS252Q02S', 'PS252Q03S', 'PS256Q01S', 'PS268Q01S', 'S268Q02', 'PS268Q06S', 'PS269Q01', 'PS269Q03', 'PS269Q04S', 'S304Q01', 'PS304Q02S', 'S304Q03a', 'S304Q03b', 'PS326Q01', 'PS326Q02', 'PS326Q03S', 'PS326Q04S', 'PS327Q01S', 'PS408Q01S', 'PS408Q03', 'PS408Q04S', 'PS408Q05S', 'PS413Q04S', 'PS413Q05S', 'PS413Q06', 'PS415Q02S', 'PS415Q07S', 'PS415Q08S', 'S416Q01', 'S421Q01', 'S421Q02', 'S421Q03', 'PS425Q02S', 'PS425Q03', 'PS425Q04', 'PS425Q05S', 'PS428Q01S', 'PS428Q03S', 'PS428Q05', 'CS437Q01S', 'CS437Q03S', 'CS437Q04S', 'DS437Q06C', 'CS438Q01S', 'CS438Q02S', 'DS438Q03C', 'DS458Q01C', 'CS458Q02S', 'DS465Q01C', 'CS465Q02S', 'CS465Q04S', 'CS466Q01S', 'CS466Q05S', 'CS466Q07S', 'CS476Q01S', 'CS476Q02S', 'CS476Q03S', 'CS478Q01S', 'CS478Q02S', 'CS478Q03S', 'CS495Q01S', 'CS495Q02S', 'DS495Q03C', 'CS495Q04S', 'CS498Q02S', 'CS498Q03S', 'DS498Q04C', 'CS510Q01S', 'DS510Q04C', 'DS514Q02C', 'DS514Q03C', 'DS514Q04C', 'DS519Q01C', 'CS519Q02S', 'DS519Q03C', 'CS521Q02S', 'CS521Q06S', 'CS524Q06S', 'DS524Q07C', 'CS527Q01S', 'CS527Q03S', 'CS527Q04S', 'PS437Q01S', 'PS437Q03S', 'PS437Q04S', 'S437Q06', 'PS438Q01S', 'PS438Q02S', 'PS438Q03', 'S458Q01', 'PS458Q02S', 'PS465Q01', 'PS465Q02S', 'PS465Q04S', 'PS466Q01S', 'PS466Q05S', 'PS466Q07S', 'PS476Q01S', 'PS476Q02S', 'PS476Q03S', 'PS478Q01S', 'PS478Q02S', 'PS478Q03S', 'PS495Q01S', 'PS495Q02S', 'S495Q03', 'PS495Q04S', 'PS498Q02S', 'PS498Q03S', 'PS498Q04', 'PS510Q01S', 'S510Q04', 'PS514Q02', 'PS514Q03', 'PS514Q04', 'PS519Q01', 'PS519Q02S', 'PS519Q03', 'PS521Q02S', 'PS521Q06S', 'PS524Q06S', 'S524Q07', 'PS527Q01S', 'PS527Q03S', 'PS527Q04S', 'CS601Q01S', 'CS601Q02S', 'CS601Q04S', 'CS602Q01S', 'CS602Q02S', 'DS602Q03C', 'CS602Q04S', 'CS603Q01S', 'DS603Q02C', 'CS603Q03S', 'CS603Q04S', 'CS603Q05S', 'CS604Q02S', 'DS604Q04C', 'CS605Q01S', 'CS605Q02S', 'CS605Q03S', 'DS605Q04C', 'CS607Q01S', 'CS607Q02S', 'DS607Q03C', 'CS608Q01S', 'CS608Q02S', 'CS608Q03S', 'DS608Q04C', 'DS610Q01C', 'CS610Q02S', 'CS610Q04S', 'CS615Q01S', 'CS615Q02S', 'CS615Q05S', 'CS615Q07S', 'CS620Q01S', 'CS620Q02S', 'DS620Q04C', 'DS625Q01C', 'CS625Q02S', 'CS625Q03S', 'CS626Q01S', 'CS626Q02S', 'CS626Q03S', 'DS626Q04C', 'CS627Q01S', 'CS627Q03S', 'CS627Q04S', 'DS629Q01C', 'CS629Q02S', 'DS629Q03C', 'CS629Q04S', 'CS634Q01S', 'CS634Q02S', 'DS634Q03C', 'CS634Q04S', 'DS634Q05C', 'CS635Q01S', 'CS635Q02S', 'DS635Q03C', 'CS635Q04S', 'DS635Q05C', 'DS637Q01C', 'CS637Q02S', 'DS637Q05C', 'CS638Q01S', 'CS638Q02S', 'CS638Q04S', 'DS638Q05C', 'CS641Q01S', 'CS641Q02S', 'CS641Q03S', 'CS641Q04S', 'CS643Q01S', 'CS643Q02S', 'DS643Q03C', 'CS643Q04S', 'DS643Q05C', 'CS645Q01S', 'CS645Q03S', 'DS645Q04C', 'DS645Q05C', 'CS646Q01S', 'CS646Q02S', 'CS646Q03S', 'DS646Q04C', 'DS646Q05C', 'DS648Q01C', 'CS648Q02S', 'CS648Q03S', 'DS648Q05C', 'CS649Q01S', 'DS649Q02C', 'CS649Q03S', 'CS649Q04S', 'CS656Q01S', 'DS656Q02C', 'CS656Q04S', 'CS657Q01S', 'CS657Q02S', 'CS657Q03S', 'DS657Q04C']\n"
     ]
    }
   ],
   "source": [
    "items = ['CNT', 'CNTSTUID']\n",
    "sheets = list(main_items.keys())\n",
    "#print(main_items[sheets[0]].columns)\n",
    "\n",
    "for s in sheets[:2]:\n",
    "    items.extend(main_items[s]['CBA item ID in main survey analysis output'].values.tolist())\n",
    "    items.extend(main_items[s]['PBA item ID in main survey analysis output'].values.tolist())\n",
    "\n",
    "for s in sheets[10:12]:\n",
    "    items.extend(main_items[s]['Item ID\\nin analysis output'].values.tolist())\n",
    "    \n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['CNT', 'CNTSTUID', 'CS601Q01S', 'CS601Q02S', 'CS601Q04S', 'CS602Q01S',\n",
    "    'CS602Q02S', 'DS602Q03C', 'CS602Q04S', 'CS603Q01S', 'DS603Q02C', \n",
    "    'CS603Q03S', 'CS603Q04S', 'CS603Q05S', 'CS604Q02S', 'DS604Q04C', \n",
    "    'CS605Q01S', 'CS605Q02S', 'CS605Q03S', 'DS605Q04C', 'CS607Q01S', \n",
    "    'CS607Q02S', 'DS607Q03C', 'CS608Q01S', 'CS608Q02S', 'CS608Q03S', \n",
    "    'DS608Q04C', 'DS610Q01C', 'CS610Q02S', 'CS610Q04S', 'CS615Q01S', \n",
    "    'CS615Q02S', 'CS615Q05S', 'CS615Q07S', 'CS620Q01S', 'CS620Q02S', \n",
    "    'DS620Q04C', 'DS625Q01C', 'CS625Q02S', 'CS625Q03S', 'CS626Q01S', \n",
    "    'CS626Q02S', 'CS626Q03S', 'DS626Q04C', 'CS627Q01S', 'CS627Q03S', \n",
    "    'CS627Q04S', 'DS629Q01C', 'CS629Q02S', 'DS629Q03C', 'CS629Q04S', \n",
    "    'CS634Q01S', 'CS634Q02S', 'DS634Q03C', 'CS634Q04S', 'DS634Q05C', \n",
    "    'CS635Q01S', 'CS635Q02S', 'DS635Q03C', 'CS635Q04S', 'DS635Q05C', \n",
    "    'DS637Q01C', 'CS637Q02S', 'DS637Q05C', 'CS638Q01S', 'CS638Q02S', \n",
    "    'CS638Q04S', 'DS638Q05C', 'CS641Q01S', 'CS641Q02S', 'CS641Q03S', \n",
    "    'CS641Q04S', 'CS643Q01S', 'CS643Q02S', 'DS643Q03C', 'CS643Q04S', \n",
    "    'DS643Q05C', 'CS645Q01S', 'CS645Q03S', 'DS645Q04C', 'DS645Q05C', \n",
    "    'CS646Q01S', 'CS646Q02S', 'CS646Q03S', 'DS646Q04C', 'DS646Q05C', \n",
    "    'DS648Q01C', 'CS648Q02S', 'CS648Q03S', 'DS648Q05C', 'CS649Q01S', \n",
    "    'DS649Q02C', 'CS649Q03S', 'CS649Q04S', 'CS656Q01S', 'DS656Q02C', \n",
    "    'CS656Q04S', 'CS657Q01S', 'CS657Q02S', 'CS657Q03S', 'DS657Q04C']\n",
    "\n",
    "#cog = pd.read_sas('PUF_SAS_COMBINED_CMB_STU_COG/cy6_ms_cmb_stu_cog.sas7bdat')\n",
    "#data_df = cog[cols]\n",
    "#data_df['CNT'] = data_df['CNT'].str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog = pd.read_spss(\n",
    "    'PUF_SPSS_COMBINED_CMB_STU_COG/CY6_MS_CMB_STU_COG.sav', usecols=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cog.copy().astype(str)\n",
    "response = np.ones(data_df.shape) * -1\n",
    "\n",
    "response[data_df.values == '0 - No credit'] = 0\n",
    "response[data_df.values == '0 - No credit'] = 0\n",
    "response[data_df.values == '01 - No credit'] = 0\n",
    "response[data_df.values == '02 - No credit'] = 0\n",
    "response[data_df.values == '03 - No credit'] = 0\n",
    "response[data_df.values == '04 - No credit'] = 0\n",
    "response[data_df.values == 'No credit'] = 0\n",
    "response[data_df.values == '1 - Full credit'] = 1\n",
    "response[data_df.values == '1 - Partial credit'] = 1\n",
    "response[data_df.values == '11 - Full credit'] = 1\n",
    "response[data_df.values == '11 - Partial credit'] = 1\n",
    "response[data_df.values == '12 - Full credit'] = 1\n",
    "response[data_df.values == '12 - Partial credit'] = 1\n",
    "response[data_df.values == '2 - Full credit'] = 1\n",
    "response[data_df.values == '21 - Full credit'] = 1\n",
    "response[data_df.values == 'Full credit'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[:, 2:] = response[:, 2:]\n",
    "data_df['CNTSTUID'] = data_df['CNTSTUID'].astype(float)\n",
    "data_df['CNT'] = data_df['CNT'].convert_dtypes(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('cog_science.csv', sep=';', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_cols = ['ST004D01T', 'ST005Q01TA', 'ST006Q01TA', 'ST006Q02TA', 'ST006Q03TA',\n",
    "            'ST006Q04TA', 'ST007Q01TA', 'ST008Q01TA', 'ST008Q02TA', 'ST008Q03TA',\n",
    "            'ST008Q04TA', 'ST011Q01TA', 'ST011Q02TA', 'ST011Q03TA', 'ST011Q04TA',\n",
    "            'ST011Q05TA', 'ST011Q06TA', 'ST011Q07TA', 'ST011Q08TA', 'ST011Q09TA',\n",
    "            'ST011Q10TA', 'ST011Q11TA', 'ST011Q12TA', 'ST011Q16NA', 'ST011D17TA',\n",
    "            'ST011D18TA', 'ST011D19TA', 'ST012Q01TA', 'ST012Q02TA', 'ST012Q03TA',\n",
    "            'ST012Q05NA', 'ST012Q06NA', 'ST012Q07NA', 'ST012Q08NA', 'ST012Q09NA',\n",
    "            'ST013Q01TA', 'ST123Q01NA', 'ST123Q02NA', 'ST123Q03NA', 'ST123Q04NA',\n",
    "            'ST019AQ01T', 'ST019BQ01T', 'ST019CQ01T', 'ST021Q01TA', 'ST022Q01TA',\n",
    "            'ST124Q01TA', 'ST125Q01NA', 'ST126Q01TA', 'ST127Q01TA', 'ST127Q02TA',\n",
    "            'ST127Q03TA', 'ST111Q01TA', 'ST118Q01NA', 'ST118Q02NA', 'ST118Q03NA',\n",
    "            'ST118Q04NA', 'ST118Q05NA', 'ST119Q01NA', 'ST119Q02NA', 'ST119Q03NA',\n",
    "            'ST119Q04NA', 'ST119Q05NA', 'ST121Q01NA', 'ST121Q02NA', 'ST121Q03NA',\n",
    "            'ST082Q01NA', 'ST082Q02NA', 'ST082Q03NA', 'ST082Q08NA', 'ST082Q09NA',\n",
    "            'ST082Q12NA', 'ST082Q13NA', 'ST082Q14NA', 'ST034Q01TA', 'ST034Q02TA',\n",
    "            'ST034Q03TA', 'ST034Q04TA', 'ST034Q05TA', 'ST034Q06TA', 'ST039Q01NA',\n",
    "            'ST039Q02NA', 'ST039Q03NA', 'ST039Q04NA', 'ST039Q05NA', 'ST039Q06NA',\n",
    "            'ST059Q01TA', 'ST059Q02TA', 'ST059Q03TA', 'ST060Q01NA', 'ST061Q01NA',\n",
    "            'ST062Q01TA', 'ST062Q02TA', 'ST062Q03TA', 'ST071Q01NA', 'ST071Q02NA',\n",
    "            'ST071Q03NA', 'ST071Q04NA', 'ST071Q05NA', 'ST031Q01NA', 'ST032Q01NA',\n",
    "            'ST032Q02NA', 'ST063Q01NA', 'ST063Q01NB', 'ST063Q02NA', 'ST063Q02NB',\n",
    "            'ST063Q03NA', 'ST063Q03NB', 'ST063Q04NA', 'ST063Q04NB', 'ST063Q05NA',\n",
    "            'ST063Q05NB', 'ST063Q06NA', 'ST063Q06NB', 'ST064Q01NA', 'ST064Q02NA',\n",
    "            'ST064Q03NA', 'ST097Q01TA', 'ST097Q02TA', 'ST097Q03TA', 'ST097Q04TA',\n",
    "            'ST097Q05TA', 'ST098Q01TA', 'ST098Q02TA', 'ST098Q03NA', 'ST098Q05TA',\n",
    "            'ST098Q06TA', 'ST098Q07TA', 'ST098Q08NA', 'ST098Q09TA', 'ST098Q10NA',\n",
    "            'ST100Q01TA', 'ST100Q02TA', 'ST100Q03TA', 'ST100Q04TA', 'ST100Q05TA',\n",
    "            'ST103Q01NA', 'ST103Q03NA', 'ST103Q08NA', 'ST103Q11NA', 'ST104Q01NA',\n",
    "            'ST104Q02NA', 'ST104Q03NA', 'ST104Q04NA', 'ST104Q05NA', 'ST107Q01NA',\n",
    "            'ST107Q02NA', 'ST107Q03NA', 'ST092Q01TA', 'ST092Q02TA', 'ST092Q04TA',\n",
    "            'ST092Q05TA', 'ST092Q06NA', 'ST092Q08NA', 'ST092Q09NA', 'ST093Q01TA',\n",
    "            'ST093Q03TA', 'ST093Q04TA', 'ST093Q05TA', 'ST093Q06TA', 'ST093Q07NA',\n",
    "            'ST093Q08NA', 'ST094Q01NA', 'ST094Q02NA', 'ST094Q03NA', 'ST094Q04NA',\n",
    "            'ST094Q05NA', 'ST095Q04NA', 'ST095Q07NA', 'ST095Q08NA', 'ST095Q13NA',\n",
    "            'ST095Q15NA', 'ST113Q01TA', 'ST113Q02TA', 'ST113Q03TA', 'ST113Q04TA',\n",
    "            'ST129Q01TA', 'ST129Q02TA', 'ST129Q03TA', 'ST129Q04TA', 'ST129Q05TA',\n",
    "            'ST129Q06TA', 'ST129Q07TA', 'ST129Q08TA', 'ST131Q01NA', 'ST131Q03NA',\n",
    "            'ST131Q04NA', 'ST131Q06NA', 'ST131Q08NA', 'ST131Q11NA', 'ST146Q01TA',\n",
    "            'ST146Q02TA', 'ST146Q03TA', 'ST146Q04TA', 'ST146Q05TA', 'ST146Q06NA',\n",
    "            'ST146Q07NA', 'ST146Q08NA', 'ST146Q09NA', 'ST076Q01NA', 'ST076Q02NA',\n",
    "            'ST076Q03NA', 'ST076Q04NA', 'ST076Q05NA', 'ST076Q06NA', 'ST076Q07NA',\n",
    "            'ST076Q08NA', 'ST076Q09NA', 'ST076Q10NA', 'ST076Q11NA', 'ST078Q01NA',\n",
    "            'ST078Q02NA', 'ST078Q03NA', 'ST078Q04NA', 'ST078Q05NA', 'ST078Q06NA',\n",
    "            'ST078Q07NA', 'ST078Q08NA', 'ST078Q09NA', 'ST078Q10NA', 'ST078Q11NA',\n",
    "            'ST065Class', 'CNTSTUID']\n",
    "\n",
    "ctx2_cols = ['ST016Q01NA', 'ST038Q01NA', 'ST038Q02NA', 'ST038Q03NA', 'ST038Q04NA', \n",
    "             'ST038Q05NA', 'ST038Q06NA', 'ST038Q07NA', 'ST038Q08NA', 'CNTSTUID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = pd.read_spss('PUF_SPSS_COMBINED_CMB_STU_QQQ/CY6_MS_CMB_STU_QQQ.sav', usecols=ctx_cols)\n",
    "ctx2 = pd.read_spss(\n",
    "    'PUF_SPSS_COMBINED_CMB_STU_QQQ/CY6_MS_CMB_STU_QQ2.sav', usecols=ctx2_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concateate all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = pd.concat([ctx, ctx2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save backup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.to_csv('ctx_science.csv', sep=';', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context data coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctx_code = pd.read_excel(\n",
    "    'Technical Report 2015 - Annex B Contrast Coding_FINAL .xlsx')\n",
    "\n",
    "ctx_code_cols = ['ST004D01T', 'ST005Q01TA', 'ST006Q01TA', 'ST006Q02TA', 'ST006Q03TA',\n",
    "            'ST006Q04TA', 'ST007Q01TA', 'ST008Q01TA', 'ST008Q02TA', 'ST008Q03TA',\n",
    "            'ST008Q04TA', 'ST011Q01TA', 'ST011Q02TA', 'ST011Q03TA', 'ST011Q04TA',\n",
    "            'ST011Q05TA', 'ST011Q06TA', 'ST011Q07TA', 'ST011Q08TA', 'ST011Q09TA',\n",
    "            'ST011Q10TA', 'ST011Q11TA', 'ST011Q12TA', 'ST011Q16NA', 'ST011D17TA',\n",
    "            'ST011D18TA', 'ST011D19TA', 'ST012Q01TA', 'ST012Q02TA', 'ST012Q03TA',\n",
    "            'ST012Q05NA', 'ST012Q06NA', 'ST012Q07NA', 'ST012Q08NA', 'ST012Q09NA',\n",
    "            'ST013Q01TA', 'ST123Q01NA', 'ST123Q02NA', 'ST123Q03NA', 'ST123Q04NA',\n",
    "            'ST019AQ01T', 'ST019BQ01T', 'ST019CQ01T', 'ST021Q01TA', 'ST022Q01TA',\n",
    "            'ST124Q01TA', 'ST125Q01NA', 'ST126Q01TA', 'ST127Q01TA', 'ST127Q02TA',\n",
    "            'ST127Q03TA', 'ST111Q01TA', 'ST118Q01NA', 'ST118Q02NA', 'ST118Q03NA',\n",
    "            'ST118Q04NA', 'ST118Q05NA', 'ST119Q01NA', 'ST119Q02NA', 'ST119Q03NA',\n",
    "            'ST119Q04NA', 'ST119Q05NA', 'ST121Q01NA', 'ST121Q02NA', 'ST121Q03NA',\n",
    "            'ST082Q01NA', 'ST082Q02NA', 'ST082Q03NA', 'ST082Q08NA', 'ST082Q09NA',\n",
    "            'ST082Q12NA', 'ST082Q13NA', 'ST082Q14NA', 'ST034Q01TA', 'ST034Q02TA',\n",
    "            'ST034Q03TA', 'ST034Q04TA', 'ST034Q05TA', 'ST034Q06TA', 'ST039Q01NA',\n",
    "            'ST039Q02NA', 'ST039Q03NA', 'ST039Q04NA', 'ST039Q05NA', 'ST039Q06NA',\n",
    "            'ST059Q01TA', 'ST059Q02TA', 'ST059Q03TA', 'ST060Q01NA', 'ST061Q01NA',\n",
    "            'ST062Q01TA', 'ST062Q02TA', 'ST062Q03TA', 'ST071Q01NA', 'ST071Q02NA',\n",
    "            'ST071Q03NA', 'ST071Q04NA', 'ST071Q05NA', 'ST031Q01NA', 'ST032Q01NA',\n",
    "            'ST032Q02NA', 'ST063Q01NA', 'ST063Q01NB', 'ST063Q02NA', 'ST063Q02NB',\n",
    "            'ST063Q03NA', 'ST063Q03NB', 'ST063Q04NA', 'ST063Q04NB', 'ST063Q05NA',\n",
    "            'ST063Q05NB', 'ST063Q06NA', 'ST063Q06NB', 'ST064Q01NA', 'ST064Q02NA',\n",
    "            'ST064Q03NA', 'ST097Q01TA', 'ST097Q02TA', 'ST097Q03TA', 'ST097Q04TA',\n",
    "            'ST097Q05TA', 'ST098Q01TA', 'ST098Q02TA', 'ST098Q03NA', 'ST098Q05TA',\n",
    "            'ST098Q06TA', 'ST098Q07TA', 'ST098Q08NA', 'ST098Q09TA', 'ST098Q10NA',\n",
    "            'ST100Q01TA', 'ST100Q02TA', 'ST100Q03TA', 'ST100Q04TA', 'ST100Q05TA',\n",
    "            'ST103Q01NA', 'ST103Q03NA', 'ST103Q08NA', 'ST103Q11NA', 'ST104Q01NA',\n",
    "            'ST104Q02NA', 'ST104Q03NA', 'ST104Q04NA', 'ST104Q05NA', 'ST107Q01NA',\n",
    "            'ST107Q02NA', 'ST107Q03NA', 'ST092Q01TA', 'ST092Q02TA', 'ST092Q04TA',\n",
    "            'ST092Q05TA', 'ST092Q06NA', 'ST092Q08NA', 'ST092Q09NA', 'ST093Q01TA',\n",
    "            'ST093Q03TA', 'ST093Q04TA', 'ST093Q05TA', 'ST093Q06TA', 'ST093Q07NA',\n",
    "            'ST093Q08NA', 'ST094Q01NA', 'ST094Q02NA', 'ST094Q03NA', 'ST094Q04NA',\n",
    "            'ST094Q05NA', 'ST095Q04NA', 'ST095Q07NA', 'ST095Q08NA', 'ST095Q13NA',\n",
    "            'ST095Q15NA', 'ST113Q01TA', 'ST113Q02TA', 'ST113Q03TA', 'ST113Q04TA',\n",
    "            'ST129Q01TA', 'ST129Q02TA', 'ST129Q03TA', 'ST129Q04TA', 'ST129Q05TA',\n",
    "            'ST129Q06TA', 'ST129Q07TA', 'ST129Q08TA', 'ST131Q01NA', 'ST131Q03NA',\n",
    "            'ST131Q04NA', 'ST131Q06NA', 'ST131Q08NA', 'ST131Q11NA', 'ST146Q01TA',\n",
    "            'ST146Q02TA', 'ST146Q03TA', 'ST146Q04TA', 'ST146Q05TA', 'ST146Q06NA',\n",
    "            'ST146Q07NA', 'ST146Q08NA', 'ST146Q09NA', 'ST076Q01NA', 'ST076Q02NA',\n",
    "            'ST076Q03NA', 'ST076Q04NA', 'ST076Q05NA', 'ST076Q06NA', 'ST076Q07NA',\n",
    "            'ST076Q08NA', 'ST076Q09NA', 'ST076Q10NA', 'ST076Q11NA', 'ST078Q01NA',\n",
    "            'ST078Q02NA', 'ST078Q03NA', 'ST078Q04NA', 'ST078Q05NA', 'ST078Q06NA',\n",
    "            'ST078Q07NA', 'ST078Q08NA', 'ST078Q09NA', 'ST078Q10NA', 'ST078Q11NA',\n",
    "            'ST065Class', 'ST016Q01NA', 'ST038Q01NA', 'ST038Q02NA', 'ST038Q03NA',\n",
    "            'ST038Q04NA', 'ST038Q05NA', 'ST038Q06NA', 'ST038Q07NA', 'ST038Q08NA']\n",
    "\n",
    "ctx_code = df_ctx_code[df_ctx_code['ITEM_ID'].isin(ctx_code_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save contrast code for students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_code.to_csv('ctx_code.csv', sep=';', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = data_df['CNT'].isin(['BRA', 'NOR'])\n",
    "cnt_df = data_df.loc[cnt]\n",
    "cnt_df = cnt_df.reset_index(drop=True)\n",
    "\n",
    "print(cnt_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = np.c_[imp, data_df['CNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = imp[cnt]\n",
    "#response = response.reset_index(drop=True)\n",
    "print(response[:, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnt_df[cnt_df['CNT'] == 'BRA'].shape)\n",
    "print(cnt_df[cnt_df['CNT'] == 'NOR'].shape)\n",
    "print(cnt_df['CNT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[response == np.nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.read_csv('cog_science.csv')\n",
    "#response = np.zeros(cnt_df.shape)\n",
    "\n",
    "response[response[:, 99] == 'NOR'] = 1\n",
    "response[response[:, 99] == 'BRA'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.shape)\n",
    "print(response[:, -1])\n",
    "#print(data_df[data_df == '0.0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = os.path.join('score_matrix.npy')\n",
    "np.save(cache_file, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(42)\n",
    "swapper = np.arange(response.shape[0])\n",
    "rs.shuffle(swapper)\n",
    "response = response[swapper]\n",
    "\n",
    "#rows_to_remove = np.sum(response, 1) == (-1 * response.shape[1])\n",
    "#rows_to_remove = np.sum(response, 1) <= -1\n",
    "#response = response[~rows_to_remove]\n",
    "#response = response[np.all(response != -1, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = pd.read_csv('Reference Matrix.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['Items', '111', '112', '113', '121', '122', '123', '131', '132', '133',\n",
    "              '211', '212', '213', '221', '222', '223', '231', '232', '233',\n",
    "              '311', '312', '313', '321', '322', '323', '331', '332', '333']\n",
    "\n",
    "q_matrix = pd.DataFrame(columns=c)\n",
    "q_matrix['Items'] = ref['Item ID in analysis output'].to_numpy()\n",
    "q_matrix.iloc[:, 1:] = 0\n",
    "#print(q_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in ref.iterrows():\n",
    "    seq = None\n",
    "    if 'Explain phenomena scientifically' in r['Competency (2015)']:\n",
    "        seq = '1'\n",
    "    elif 'Interpret data and evidence scientifically' in r['Competency (2015)']:\n",
    "        seq = '2'\n",
    "    elif 'Evaluate and design scientific enquiry' in r['Competency (2015)']:\n",
    "        seq = '3'\n",
    "\n",
    "    if 'Content' in r['Knowledge (2015)']:\n",
    "        seq += '1'\n",
    "    elif 'Procedural' in r['Knowledge (2015)']:\n",
    "        seq += '2'\n",
    "    elif 'Epistemic' in r['Knowledge (2015)']:\n",
    "        seq += '3'\n",
    "\n",
    "    if 'Physical' in r['System (2015)']:\n",
    "        seq += '1'\n",
    "    elif 'Living' in r['System (2015)']:\n",
    "        seq += '2'\n",
    "    elif 'Earth and Space' in r['System (2015)']:\n",
    "        seq += '3'\n",
    "    \n",
    "    q_matrix.loc[q_matrix.index[i], seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_matrix.to_csv('q_matrix_PISA15.csv', sep=';', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = q_matrix.iloc[:,1:].T\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = os.path.join('test_response.npy')\n",
    "\n",
    "if os.path.isfile(test_response):\n",
    "    response = np.load(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict connection in decoder\n",
    "def q_constraint(w):\n",
    "    target = w * Q\n",
    "    diff = w - target\n",
    "    w = w * tf.cast(tf.math.equal(diff, 0), keras.backend.floatx()) \n",
    "    return w * tf.cast(tf.math.greater_equal(w, 0), keras.backend.floatx())\n",
    "\n",
    "# Remove zeros function\n",
    "def remove_zeros(arr):\n",
    "  n_arr = []\n",
    "  \n",
    "  for j in range(num_skills): \n",
    "    for i in range(num_stats):\n",
    "      if Q.iloc[j, i] != 0:\n",
    "        n_arr.append(arr[j][i])\n",
    "  \n",
    "  return n_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stats and skills\n",
    "num_stats = 99 #180\n",
    "num_skills = 27 #21\n",
    "\n",
    "intermediate_dim=40\n",
    "\n",
    "# Number of subjects\n",
    "N = response.shape[0]\n",
    "# Training number\n",
    "tr = cnt_df[cnt_df['CNT'] == 'BRA'].shape[0]\n",
    "batch_size = response.shape[0]#50\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    #epsilon = tfp.distributions.normal(0, 1)\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "    \"\"\"Maps items respone to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=num_skills, intermediate_dim=intermediate_dim, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"tanh\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        \n",
    "        self.dense_mean_POP = layers.Dense(latent_dim, use_bias=False)\n",
    "        self.dense_log_var_POP = layers.Dense(latent_dim, use_bias=False)\n",
    "        \n",
    "        self.dense_mean_added = layers.Add()\n",
    "        self.dense_log_var_added = layers.Add()\n",
    "        \n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(inputs.shape)\n",
    "\n",
    "        # Items response\n",
    "        x1 = self.dense_proj(inputs[:,:-1])\n",
    "        z_mean = self.dense_mean(x1)\n",
    "        z_log_var = self.dense_log_var(x1)\n",
    "\n",
    "        # Population\n",
    "        z_mean_pop = self.dense_mean_POP(inputs[:,-1:])\n",
    "        z_log_var_pop = self.dense_log_var_POP(inputs[:,-1:])\n",
    "    \n",
    "        z_mean_added = self.dense_mean_added([z_mean, z_mean_pop])\n",
    "        z_log_var_added = self.dense_log_var_added([z_log_var, z_log_var_pop])\n",
    "\n",
    "        z = self.sampling((z_mean_added, z_log_var_added))\n",
    "        return z_mean, z_log_var, z, z_mean_added, z_log_var_added\n",
    "\n",
    "\n",
    "class Decoder(keras.Model):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, latent_dim=num_skills, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        #self.dense_proj = layers.Dense(latent_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim\n",
    "                                         , activation=\"sigmoid\"\n",
    "                                         , kernel_constraint=q_constraint\n",
    "                                         #, kernel_initializer=initializers.Ones()\n",
    "                                         #, bias_initializer=initializers.Zeros()\n",
    "                                         )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense_output(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        latent_dim=num_skills,\n",
    "        name=\"autoencoder\"\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, latent_dim=latent_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.z_mean, self.z_log_var, self.z, _, _ = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(self.z)\n",
    "    \n",
    "        #print(inputs.shape)\n",
    "        #print(reconstructed.shape)\n",
    "        return reconstructed\n",
    "\n",
    "    # Loss function\n",
    "    def vae_loss(self, input, output):\n",
    "        \n",
    "        #TODO: New loss function\n",
    "        #func_a = self.decoder.trainable_weights\n",
    "        \n",
    "        cross_entropy_loss = (num_stats / 1.0) * keras.losses.binary_crossentropy(input[:,:-1], output)\n",
    "        \n",
    "        kl_loss = -0.5 * tf.reduce_mean(self.z_log_var - tf.square(self.z_mean) - tf.exp(self.z_log_var) + 1, axis=-1)\n",
    "        \n",
    "        return cross_entropy_loss + kl_loss * (1 - input[:, -1:])\n",
    "\n",
    "    # Get weights\n",
    "    def _get_weights(self):\n",
    "        return self.decoder.trainable_weights\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Binary Accuracy Metric\n",
    "# https://github.com/keras-team/keras/blob/1d12a9287cbe3f2d4cdde84918f0ef2086da9bee/keras/metrics.py#L3373\n",
    "\n",
    "def binary_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    threshold = tf.cast(threshold, y_pred.dtype)\n",
    "    y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n",
    "    return keras.backend.mean(tf.equal(y_true[:,:-1], y_pred), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dtrain):\n",
    "    vae_q = VariationalAutoEncoder(num_stats, intermediate_dim, num_skills)\n",
    "\n",
    "    # Optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.005, amsgrad=True)\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=0.005)\n",
    "    \n",
    "    vae_q.compile(optimizer=opt, loss=vae_q.vae_loss, metrics=[binary_accuracy])\n",
    "\n",
    "    history = vae_q.fit(dtrain,\n",
    "                        dtrain,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "    # validation_split=0.2\n",
    "    \n",
    "    ########################## Binary Accuracy Mean ############################\n",
    "    #ba = 0\n",
    "    #for value in history.history['binary_accuracy']:\n",
    "    #    ba += value\n",
    "    #print(\"Binary Accuracy: %.4f\" % (ba / 25))\n",
    "    ############################################################################\n",
    "\n",
    "    encoder = vae_q.get_encoder()\n",
    "    decoder = vae_q.get_decoder()\n",
    "    #weights = vae_q.get_decoder().trainable_weights\n",
    "\n",
    "    weights = vae_q._get_weights()\n",
    "\n",
    "    discr = weights[0].numpy()\n",
    "    #print(discr.shape)\n",
    "    #diff = pd.DataFrame(weights[3].numpy())\n",
    "    negative_diff = pd.DataFrame(np.negative(weights[1].numpy()))\n",
    "\n",
    "    # Get latent trait predictions\n",
    "    z_mean_pred, z_logvar_pred, z_pred, z_mean_added_pred, z_logvar_added_pred = encoder.predict(dtrain)\n",
    "    output_pred = decoder.predict(z_pred)\n",
    "\n",
    "    s = Sampling()\n",
    "    pv = [s([z_mean_added_pred, z_logvar_added_pred]) for i in range(5)]\n",
    "\n",
    "    output_array = [decoder.predict(i) for i in pv]\n",
    "\n",
    "    #bern_array = sc.stats.bernoulli.rvs(output_array)\n",
    "\n",
    "\n",
    "    #print(pred)\n",
    "    #print(thetas_hat)\n",
    "\n",
    "    # Total score on the test -------\n",
    "    score = np.apply_over_axes(np.sum, dtrain, 1)\n",
    "\n",
    "    #### Vectoring the matrices Thetas_hat ans discr ####\n",
    "    theta_hat = np.transpose(z_mean_pred).flatten()\n",
    "    #step_theta_hat = np.transpose(step_thetas_hat.numpy()).flatten()\n",
    "  \n",
    "    log_var_theta_hat = np.transpose(z_logvar_pred).flatten()\n",
    "    #step_log_var_theta_hat = np.transpose(step_log_var_thetas_hat.numpy()).flatten()\n",
    "\n",
    "    discr_hat = remove_zeros(discr)\n",
    "\n",
    "    return {\n",
    "        'z_mean_pred': z_mean_pred,\n",
    "        'z_logvar_pred': z_logvar_pred, \n",
    "        'z': z_pred,\n",
    "        'z_mean_added_pred': z_mean_added_pred,\n",
    "        'z_logvar_added_pred': z_logvar_added_pred,\n",
    "        'theta_hat': theta_hat,\n",
    "        'logvar_theta_hat': log_var_theta_hat,\n",
    "        'discr_hat': discr_hat,\n",
    "        'diff': negative_diff,\n",
    "        'output': output_pred,\n",
    "        'weights': encoder.trainable_weights,\n",
    "        'output_array': output_array\n",
    "        #'bern_array': bern_array\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[:, -1:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tf.cast(response, tf.float32)\n",
    "results = train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aa822445277b5d7e3b644e2bff57edb38bf7124dc1aeb3c0dedced1e7b4036d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
